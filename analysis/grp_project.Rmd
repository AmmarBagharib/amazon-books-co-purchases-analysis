---
title: "Group Project (exploration)"
author: "Chen Yang"
date: "2023-09-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(igraph)
library(data.table)
library(glue)
library(gridExtra)
library(ggplot2)
library(glue)
library(stringr)
library(here)
```


```{r , setup, include=FALSE}
#set working directory to project root folder
knitr::opts_knit$set(root.dir = here())
```


```{r metadata}
data_path <- "../data/"

#read metadata txt
meta <- readLines(paste0(data_path, "amazon-meta.txt.gz"))

#exclude blanks, reviews, actor/actress info so that we deal with less data
meta <- meta[meta != "" & !grepl("^\\s*\\d+-\\d+-\\d+\\s*cutomer:\\s*[A-Za-z0-9]+", meta) & !grepl("\\|Actors & Actresses\\[\\d+\\]\\|[^|]*", meta)]

#define regex pattern and identify item id, start/stop index for each item
pattern <- "^\\s*[iI]+\\s*[dD]+\\s*:+\\s*\\d+$"
start_ind <- which(grepl(pattern, meta))
stop_ind <- c(start_ind[2:length(start_ind)]-1, length(meta))

#function to handle empty ID matches
conditional_fn <- function(x) {
  ifelse(length(x) == 0, NA, x)
}

#get item ids with regex
item_id <- meta[start_ind] %>% regmatches(., gregexpr("\\d+", .)) %>% lapply(., conditional_fn) %>% unlist()


#compile into dataframe
item_index <- data.frame(id = item_id, start = start_ind, stop = stop_ind, length = stop_ind - start_ind + 1)

#get product type
grp_pattern <- "group:\\s*(.+)"
product_type <- c()

for (i in 1:length(item_index %>% row.names())) {
  test_item <- meta[item_index$start[i]:item_index$stop[i]]
  type <- test_item %>% .[grep(grp_pattern, .)] %>% str_match(., grp_pattern) %>% .[2]
  product_type <- c(product_type, type)
}

item_index$type <- product_type

#view distribution of attribute length
item_index %>% group_by(length) %>% summarize(n = n())

#save
#write.csv(item_index, "item_index.csv", row.names = FALSE)
```

```{r extract_genre_from_books}
#create another dataframe which only contain books
item_index_books <- item_index[item_index$type == "Book",] %>% drop_na()

###Extract book genres - the data is formatted such that each book has a main genre, followed by a bunch of sub-genres. 
### Each book can belong in multiple main and sub-genres. For simplicity, we'll only be looking at the main genre as there are too many sub-genres

#define regex pattern for elements containing genre info
genre_pattern <- "\\|\\w+\\[\\d+\\]\\|"

#empty list to hold extracted genres and the re-concatenated genre string
main_genres <- c()
cleaned_genres <- c()

#elements to exclude as they are not genres
non_genre_elems<- c(
  "Books", "Subjects", "Specialty Stores", 
  "Formats", "Amazon.com Stores", "jp-unknown2", 
  "jp-unknown3","jp-unknown1", "By Publisher", 
  "O'Reilly", "Categories","John Wiley & Sons",
  "VHS", "DVD", ""
)

#iterate over every book
for (i in 1:length(row.names(item_index_books))) {
  if (i %% 1000 == 0) {
    print(glue("processed {i} entries"))
  }
  #select an item's character vector and trim white spaces on either side of each element
  char_vec <- meta[item_index_books$start[i]:item_index_books$stop[i]] %>% trimws()
  
  #extract and clean genre using regex
  genres_dirty <- char_vec[which(grepl(genre_pattern, char_vec))] %>% #select the elements which contain genre info
    paste(., collapse = "") %>% #combine them together using paste
    strsplit("\\|") %>% #split into elements using "|" as delimiter
    unlist() %>% #unlist
    str_replace_all(., "\\[\\d+\\]", "") #remove genre and sub-genre ID
  
  #remove non-genre elements from list
  genres_clean <- genres_dirty %>% unique() %>% .[!(. %in% non_genre_elems)]
  
  cleaned_genres <- c(cleaned_genres, paste(genres_clean, collapse = "|"))
  
  #add main genre to main_genres vector if it's not already inside
  main_genres <- c(main_genres, genres_clean[1][!(genres_clean[1] %in% main_genres)])
}

#remove NA elements
main_genres <- main_genres[!(is.na(main_genres))]
main_genres[!(main_genres %in% non_genre_elems)]

#check if number of cleaned genres equal to number of rows in books dataframe
item_index_books$cleaned_genres <- cleaned_genres
#save csv
#write.csv(item_index_books, "item_index_books.csv", row.names = FALSE)
```


```{r genre_one_hot_encoding}
#one-hot encoding function
OHE_genre <- function(main_genres, str) {
  op <- lapply(main_genres, function (x) {grepl(x, str)}) %>% unlist() %>% as.numeric()
  return(op)
}

#one-hot encoding for genre
encoded_data <- lapply(cleaned_genres, function(x) {OHE_genre(main_genres, x)})
encoded_data <- encoded_data %>% as.data.frame() %>% transpose()
colnames(encoded_data) <- main_genres

#bind column-wise
item_index_books <- cbind(item_index_books, encoded_data)

#save csv
write.csv(item_index_books, "item_index_books.csv", row.names = FALSE)

#preview
item_index_books
```


```{r read_graph_data}
set.seed(123)
df <- read.table('Amazon0302.txt')
g1 <- graph_from_data_frame(df, directed = TRUE)
```

```{r deg_visualization}
in_deg = degree(g1, mode = "in")
in_deg = data.frame(deg = in_deg)

ggplot(in_deg, aes(x = deg)) +
  geom_histogram(bins = 20) +
  scale_y_continuous(trans = "log10", breaks = c(1, 10, 100, 1000, 100000)) +
  labs(y = 'freq (log10)', x = 'in degree')
```

```{r sampling}
in_deg <- in_deg %>% mutate(bin = cut(deg, breaks = 100, labels = FALSE))

sampled_df <- in_deg %>%
  group_by(bin) %>%
  sample_frac(0.0001, replace = FALSE) %>%
  ungroup()

#get subgraph from sampled vertices
subg <- induced_subgraph(g1, row.names(sampled_df))

#get largest component
comps <- clusters(subg)
largest_comp_ind <- which.max(comps$csize)
largest_comp_vert <- which(comps$membership == largest_comp_ind)

plot(induced_subgraph(g1, largest_comp_vert), layout = layout.fruchterman.reingold(subg))
``


