---
title: "Group Project (exploration)"
author: "group"
date: "2023-09-15"
output: html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(igraph)
library(data.table)
library(glue)
library(gridExtra)
library(ggplot2)
library(stringr)
library(here)
library(caret)
library(pROC)
library(glmnet)
```

```{r , setup, include=FALSE}
#set working directory to project root folder
knitr::opts_knit$set(root.dir = here())
```

# Project Background

Established in 1995 as a pioneering online bookstore, Amazon has been a cherished destination for book enthusiasts for several decades. While the company has evolved and expanded into a diverse range of industries, it has consistently maintained its status as a go-to hub for book shoppers.

In the scope of this project, our objective is to delve into the evolving co-purchase patterns of books over time, understand it's characteristics and underlying patterns. With these information, we'll then develop a model that can provide personalized book recommendations based on books which the customer has added to his/her basket.

In doing so, we anticipate that our efforts will yield a mutually beneficial outcome. By facilitating book discoveries and guiding readers to titles they might not have encountered otherwise, we aim to stimulate sales on the platform, benefiting both Amazon and its thriving community of book lovers.

**1. Network Characteristics**

Note:

On our end, the metadata is collected in summer 2006 (possible dates is Wed, 21 Jun 2006 â€“ Sat, 23 Sept 2006), and our co-purchase data: `amazon0312` was collected in March 02 2003.

The number of months from the midpoint of summer, 7 August 2006, to March 02 2003 is 30 months 3 weeks, thus we round up to 31 months.

we've filtered for such book-to-book co-purchases:
1. With average reviews > 1 in 31 month period (for both books)
2. in a co-purchase, both books have a valid genre i.e., not an empty string

```{r load-models-and-test}
set.seed(123)
######################################################################
# test data using 0302 network metrics
# test data
test_0601_0302 <- read.csv(here('outputs/scaled_test_0302.csv'))

# transitivity and closeness related columns have NA values. Replace them with -1 value.
test_0601_0302$from_transitivity[is.na(test_0601_0302$from_transitivity)] <- -1
test_0601_0302$to_transitivity[is.na(test_0601_0302$to_transitivity)] <- -1
test_0601_0302$from_closeness[is.na(test_0601_0302$from_closeness)] <- -1
test_0601_0302$to_closeness[is.na(test_0601_0302$to_closeness)] <- -1

####0302
# model 1: baseline features only
model1_0302 <- readRDS(here("outputs/ml_models/scaled_logistic_model1_0302.rds"))

test_1_0302 <- test_0601_0302[colnames(test_0601_0302)[3:10]]

# model 2: network metrics only
model2_0302 <- readRDS(here("outputs/ml_models/scaled_logistic_model2_0302.rds"))

test_2_0302 <- test_0601_0302[colnames(test_0601_0302)[c(3, c(11:23))]]

# model 3: baseline features + network metrics
model3_0302 <- readRDS(here("outputs/ml_models/scaled_logistic_model3_0302.rds"))

test_3_0302 <- test_0601_0302[colnames(test_0601_0302)[3:23]]

######################################################################
# test data using 0505 network metrics
####0302
test_0601_0505 <- read.csv(here('outputs/scaled_test_0505.csv'))

# transitivity and closeness related columns have NA values. Replace them with -1 value.
test_0601_0505$from_transitivity[is.na(test_0601_0302$from_transitivity)] <- -1
test_0601_0505$to_transitivity[is.na(test_0601_0302$to_transitivity)] <- -1
test_0601_0505$from_closeness[is.na(test_0601_0302$from_closeness)] <- -1
test_0601_0505$to_closeness[is.na(test_0601_0302$to_closeness)] <- -1

# model 1: baseline features only
model1_0505 <- readRDS(here("outputs/ml_models/scaled_logistic_model1_0505.rds"))

test_1_0505 <- test_0601_0302[colnames(test_0601_0505)[3:10]]

# model 2: network metrics only
model2_0505 <- readRDS(here("outputs/ml_models/scaled_logistic_model2_0505.rds"))

test_2_0505 <- test_0601_0302[colnames(test_0601_0505)[c(3, c(11:23))]]

# model 3: baseline features + network metrics
model3_0505 <- readRDS(here("outputs/ml_models/scaled_logistic_model3_0505.rds"))

test_3_0505 <- test_0601_0505[colnames(test_0601_0505)[3:23]]

```

```{r function_accuracy}
# helper function to calculate accuracy
calculate_accuracy <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data, type = "response")
 # print(head(predictions))  # Print the first few predictions for debugging
  predicted_classes <- ifelse(predictions > 0.5, 1, 0)
  #print(predicted_classes)  # Print the first few predicted classes for debugging
  accuracy <- mean(predicted_classes == test_data$connected, na.rm = TRUE)
  return(accuracy)
}

model_names <- list("baseline 0302", "network metrics 0302", "baseline + network metrics 0302", 
                "baseline 0505", "network metrics 0505", "baseline + network metrics 0505")

model_list <- list(model1_0302, model2_0302, model3_0302, 
                model1_0505, model2_0505, model3_0505)

test_list <- list(test_1_0302, test_2_0302, test_3_0302,
               test_1_0505, test_2_0505, test_3_0505)
```


```{r models_accuracy}
# Create an empty dataframe to store results
results_df <- data.frame(Model = character(), Accuracy = numeric(), stringsAsFactors = FALSE)

# Loop over models and tests
for (i in seq_along(model_list)) {
  model <- model_list[[i]]
  test_data <- test_list[[i]]
  model_name <- model_names[[i]]
  
  # Calculate accuracy using the helper function
  accuracy <- calculate_accuracy(model, test_data)
  #print(accuracy)
  #print(model_name)
  
  results_row <- data.frame(Model = model_name, Accuracy = accuracy)
  # Store results in the dataframe
  results_df <- bind_rows(results_df, results_row)
}

# Print or inspect the results dataframe
results_df

```


```{r coefficients}

```



```{r creating_baseline_model}
# First create a baseline model: Logistic regression. It is chosen for its simplicity and easy interpretability. Come up with a baseline model first without any feature selection or hyperparameter tuning first and see performance.



# Summary of the model
summary(baseline_logit_model)
```



```{r baseline_model_performance}


```


```{r node_attributes}

```

```{r merging_baseline_dataset_node_attributes}

```

```{r merged_model_training}

```


```{r merging_baseline_dataset_node_attributes_test_set}

```


```{r merged_model_performance}

```

```{r LASSO_feature_selection_hyperparameter_tuning}

set.seed(42)

# shuffle the code first before CV because currently, the dataset is order in the way where it is all negative class then positive class
merged_data <- merged_data[sample(nrow(merged_data)), ]
x <- as.matrix(merged_data[, -which(names(merged_data) == "is_connected")])
y <- merged_data$is_connected

# Fit the regularized LASSO regression model
# Here, 'alpha' is set to 1 for LASSO
# use cv.glmnet to perform cross-validation to find the optimal lambda value. Default nfolds = 10.
cv_fit <- cv.glmnet(x, y, family = "binomial", alpha = 1)
plot(cv_fit)

# optimal lambda
best_lambda <- cv_fit$lambda.min
cat("Best lambda from cross-validation:", best_lambda, "\n")

# tuned model
tuned_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best_lambda)

lasso_coefficients <- coef(tuned_model, s = best_lambda)
```

```{r tuned_model_predictions}
x_test <- as.matrix(merged_test_data_positive[, -which(names(merged_test_data_positive) == "is_connected")])

y_test <- merged_test_data_positive$is_connected

tuned_preds <- ifelse(predict(tuned_model, newx = x_test, type = "respons", s = best_lambda) > 0.5, 1, 0)

# accuracy
tuned_accuracy = sum(tuned_preds == 1) / nrow(merged_test_data_positive)
# Print the accuracy
print(paste("Accuracy of baseline model", tuned_accuracy))
```

```{r merging_node_attributes_books_df}
merged_df = books_df %>% mutate(id = as.character(id)) %>% 
  left_join(node_attributes_df, by = c("id" = "vertex_id"))
head(merged_df)
```


***Network Features in March***

**2.1. Network Sampling**

To enable us to compute some of the key network metrics efficiently, we opted to randomly sample 100,000 nodes from the network. This approach offers a practical means of analysis while retaining a substantial portion of the network's complexity. As we can see in the figure below, the subgraph formed by our sampled nodes exhibits a degree and transitivity distribution that closely mirrors those of the original network. This congruence strongly suggests that the sampled network effectively captures the essential structural characteristics of the larger network.

```{r compare_structure,  fig.width=10, fig.height=5}
#load network metrics for sampled 0302
sampled_metrics <- read.csv(here("outputs/network_metrics_0302.csv"))

#load original network and calculate deg and local transitivity
original_0302 <- read.table("../data/Amazon0302.txt")
original_0302_graph <- simplify(graph_from_data_frame(original_0302, directed = FALSE))
original_deg <- degree(original_0302_graph)
original_trans <- transitivity(original_0302_graph, type = "local")

#visualize distributions
sampled_gr_deg <- ggplot(sampled_metrics, aes(x = degree)) + 
  geom_histogram(bins = 7) + 
  scale_y_log10() + 
  scale_x_log10() +
  labs(title = "Sampled Graph", x = "log(Degree Centrality)", y = "log(Frequency)")
orig_gr_deg <- ggplot() + 
  geom_histogram(bins = 7, aes(x = original_deg)) + 
  scale_y_log10() +
  scale_x_log10() +
  labs(title = "Original Graph", x = "log(Degree Centrality)", y = "log(Frequency)")

sampled_gr_trans <- ggplot(sampled_metrics, aes(x = transitivity)) + 
  geom_histogram(bins = 7) + 
  scale_y_log10() +
  labs(title = "Sampled Graph", x = "Local Transitivity", y = "log(Frequency)")
orig_gr_trans <- ggplot() + 
  geom_histogram(bins = 7, aes(x = original_trans)) + 
  scale_y_log10() +
  labs(title = "Original Graph", x = "Local Transitivity", y = "log(Frequency)")


#plot
grid.arrange(sampled_gr_deg, orig_gr_deg, sampled_gr_trans, orig_gr_trans, ncol = 2)
```


**2.2. Degree Centrality**

Majority of books in our network have low degree centrality, with approximately 3 or fewer connections. On the far right of the distribution, we observe a small group of books with approximately 70 first-degree connections, indicating a significant influence on the purchase of other books.
This suggests that most books within the network neither strongly influence the purchase of other books nor are significantly influenced by other books. Examining these high in-degree centrality outliers can provide valuable insights, which could allow us to formulate effective cross-selling strategies that enhance co-purchase tendencies.

```{r deg_visualization, fig.width=10, fig.height=5}
ggplot(sampled_metrics, aes(x = degree)) + 
  geom_histogram() + 
  labs(title = "Sampled Graph - Degree Distribution", x = "Degree Centrality", y = "log(Frequency)")
```

**2.3. Transitivity and Density**
With a global transitivity of 0.236, our co-purchase network demonstrates a moderate level of clustering, indicating that some books do tend to form clusters in terms of co-purchase behaviour.

Simultaneously, the network exhibits a low density of 0.0000359, signifying a sparse web of connections between products. This low density indicates that even though a small number of clusters exist, the overall network is not densely interconnected.

Taken together, this means that books within the network are not frequently co-purchased together, and individual products seem to have limited influence on the purchasing behaviour of others. This disconnectedness further highlights the challenge in seeking out cross-selling opportunities, as there are few evident connections or influences between various types of books.

```{r density_trans_original}
global_transitivity = transitivity(original_0302_graph, type="global")
density = graph.density(original_0302_graph)
print(paste("global transitivity:", global_transitivity, "| density:", density))
```

**2.4. PageRank**
```{r pagerank, fig.width = 10, fig.height = 5}
#visualize distribution of pagerank
ggplot(sampled_metrics) + 
  geom_histogram(aes(x = pagerank)) + 
  scale_y_log10() +
  labs(title = "Sampled Graph - Pagerank", x = "Pagerank", y = "log(Frequency)")
```


```{r pagerank_top_genres}
#load book-specific features
books_info <- read.csv(here("data/item_index_books.csv"))

#left join books_info to sampled_metrics
sampled_metrics <- sampled_metrics %>% left_join(., books_info, by = c("vertex_name"="id")) %>% arrange(desc(pagerank)) %>% distinct()

#view top 20 books' pagerank
sampled_metrics %>% head(20) %>% .[,c("vertex_name", "pagerank")]
top_genres <- c()

for (i in 1:100) {
  vec = sampled_metrics$cleaned_genres[i] %>% strsplit(., "\\|")
  top_genres <- c(top_genres, vec) %>% unlist()
}

#count genres in top 100 pagerank books
res <- top_genres %>% table() %>% as.data.frame()
colnames(res) <- c("Genre", "Count")
res %>% arrange(desc(Count)) %>% .[3:13,]
```

Source Codes

https://cran.r-project.org/web/packages/linkprediction/linkprediction.pdf

https://rpubs.com/writetosamadalvi/CommunityDetection

https://users.dimi.uniud.it/~massimo.franceschet/R/communities.html

https://rstudio-pubs-static.s3.amazonaws.com/734940_93adb495f0e34ca291fcda1d214129d1.html#Service_as_a_Freelancer









