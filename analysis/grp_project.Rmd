---
title: "Group Project (exploration)"
author: "group"
date: "2023-09-15"
output: html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(igraph)
library(data.table)
library(glue)
library(gridExtra)
library(ggplot2)
library(stringr)
library(here)
library(caret)
library(pROC)
library(glmnet)
```

```{r , setup, include=FALSE}
#set working directory to project root folder
knitr::opts_knit$set(root.dir = here())
```

# Project Background

Established in 1995 as a pioneering online bookstore, Amazon has been a cherished destination for book enthusiasts for several decades. While the company has evolved and expanded into a diverse range of industries, it has consistently maintained its status as a go-to hub for book shoppers.

In the scope of this project, our objective is to delve into the evolving co-purchase patterns of books over time, understand it's characteristics and underlying patterns. With these information, we'll then develop a model that can provide personalized book recommendations based on books which the customer has added to his/her basket.

In doing so, we anticipate that our efforts will yield a mutually beneficial outcome. By facilitating book discoveries and guiding readers to titles they might not have encountered otherwise, we aim to stimulate sales on the platform, benefiting both Amazon and its thriving community of book lovers.

**1. Network Characteristics**

Note:

On our end, the metadata is collected in summer 2006 (possible dates is Wed, 21 Jun 2006 â€“ Sat, 23 Sept 2006), and our co-purchase data: `amazon0312` was collected in March 02 2003.

The number of months from the midpoint of summer, 7 August 2006, to March 02 2003 is 30 months 3 weeks, thus we round up to 31 months.

we've filtered for such book-to-book co-purchases:
1. With average reviews > 1 in 31 month period (for both books)
2. in a co-purchase, both books have a valid genre i.e., not an empty string

```{r load-models}
set.seed(123)
# model 1: baseline features only
model1_0302 <- readRDS(here("outputs/ml_models/logistic_model1_0302.rds"))

# model 2: network metrics only
model2_0302 <- readRDS(here("outputs/ml_models/logistic_model2_0302.rds"))

# model 3: baseline features + network metrics
model3_0302 <- readRDS(here("outputs/ml_models/logistic_model3_0302.rds"))

# model 1: baseline features only
model1_0505 <- readRDS(here("outputs/ml_models/logistic_model1_0505.rds"))

# model 2: network metrics only
model2_0505 <- readRDS(here("outputs/ml_models/logistic_model2_0505.rds"))

# model 3: baseline features + network metrics
model3_0505 <- readRDS(here("outputs/ml_models/logistic_model3_0505.rds"))

# test data
test_0601_0302 <- read.csv(here('outputs/baseline_0601_and_0302_network_metrics.csv'))
```

```{r models_accuracy}

```


```{r coefficients}

```



```{r creating_baseline_model}
# First create a baseline model: Logistic regression. It is chosen for its simplicity and easy interpretability. Come up with a baseline model first without any feature selection or hyperparameter tuning first and see performance.



# Summary of the model
summary(baseline_logit_model)
```



```{r baseline_model_performance}


```


```{r node_attributes}

```

```{r merging_baseline_dataset_node_attributes}

```

```{r merged_model_training}

```


```{r merging_baseline_dataset_node_attributes_test_set}

```


```{r merged_model_performance}

```

```{r LASSO_feature_selection_hyperparameter_tuning}

set.seed(42)

# shuffle the code first before CV because currently, the dataset is order in the way where it is all negative class then positive class
merged_data <- merged_data[sample(nrow(merged_data)), ]
x <- as.matrix(merged_data[, -which(names(merged_data) == "is_connected")])
y <- merged_data$is_connected

# Fit the regularized LASSO regression model
# Here, 'alpha' is set to 1 for LASSO
# use cv.glmnet to perform cross-validation to find the optimal lambda value. Default nfolds = 10.
cv_fit <- cv.glmnet(x, y, family = "binomial", alpha = 1)
plot(cv_fit)

# optimal lambda
best_lambda <- cv_fit$lambda.min
cat("Best lambda from cross-validation:", best_lambda, "\n")

# tuned model
tuned_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best_lambda)

lasso_coefficients <- coef(tuned_model, s = best_lambda)
```

```{r tuned_model_predictions}
x_test <- as.matrix(merged_test_data_positive[, -which(names(merged_test_data_positive) == "is_connected")])

y_test <- merged_test_data_positive$is_connected

tuned_preds <- ifelse(predict(tuned_model, newx = x_test, type = "respons", s = best_lambda) > 0.5, 1, 0)

# accuracy
tuned_accuracy = sum(tuned_preds == 1) / nrow(merged_test_data_positive)
# Print the accuracy
print(paste("Accuracy of baseline model", tuned_accuracy))
```

```{r merging_node_attributes_books_df}
merged_df = books_df %>% mutate(id = as.character(id)) %>% 
  left_join(node_attributes_df, by = c("id" = "vertex_id"))
head(merged_df)
```

**2.1. Degree Centrality**

The degree distribution of our network exhibits a power-law distribution, a characteristic often seen in naturally occurring networks. Majority of books in our network have low degree centrality, typically three or fewer connections. On the far right of the distribution, we observe a small group of books with 12 or more first-degree connections, indicating a significant influence on the purchase of other books.

This suggests that most books within the network neither strongly influence the purchase of other books nor are significantly influenced by other books. Examining these high in-degree centrality outliers can provide valuable insights, which could allow us to formulate effective cross-selling strategies that enhance co-purchase tendencies.

```{r deg_visualization, fig.width=15, fig.height=5}
#function to plot histogram
plot_hist <- function(g, title, x_label, y_label, log_scale = FALSE) {
  labels <- labs(y = y_label, x = x_label, title = title)  # Added title parameter
  graph <- ggplot() + geom_histogram(aes(x = g), bins = 10) + labels
  
  if (log_scale == TRUE) {
    return(graph + ls)
  } else {
    return(graph)
  }
}

#calculate in, out and all degree centrality for each graph
deg <- degree(g)

#visualize
plot_hist(deg, "degree distribution for 0302", "degree", "frequency")
```

**2.2. Degree Centrality**
The low network density of 0.000598 in the network indicates that, relative to the total possible connections between books, there are very few actual co-purchases. This suggests that the network is quite sparse, with a limited number of co-purchases taking place.

Global transitivity measures how likely the neighbors of two connected nodes are connected to each other. In the case of our co-purchase network, it measures the likelihood that the alters of two books which are co-purchased together are also co-purchased with each other. The moderate global transitivity value of 0.242 tells us that there is a noteworthy degree of clustering or transitivity within this sparse network. 

This means that, despite the overall sparsity, there is a tendency for co-purchased books to have shared co-purchases with other books in the network. In simpler terms, when two books are co-purchased together, there is a relatively higher likelihood that other books co-purchased with those two are also co-purchased with each other. 

This suggests that there are distinct co-purchase patterns or niche markets within the network. Products within these clusters frequently co-purchase with each other, even if the overall network is not densely interconnected.

```{r graph-attributes}
global_transitivity = transitivity(g, type="global")
density = graph.density(g)
print(paste("The global transitivity of the graph is", global_transitivity, "and the graph density is", density))
```

# Clusters

```{r component, fig.width = 15, fig.height = 10}
#get largest component
comps <- clusters(g)
largest_comp_ind <- which.max(comps$csize)
largest_comp_vert <- which(comps$membership == largest_comp_ind)
largest_comp_subg <- induced_subgraph(g, largest_comp_vert)

#visualize largest component
plot(largest_comp_subg, layout = layout.fruchterman.reingold(g), vertex.size = degree(largest_comp_subg))
#degree distribution for sampled graph
plot_hist(degree(g), "Degree Centrality distribution for sampled graph", "degree centrality", "frequency", log_scale = FALSE)
#degree distribution for largest component
plot_hist(degree(largest_comp_subg), "Degree Centrality distribution for largest component", "degree centrality", "frequency", log_scale = FALSE)
```


# Clustering

Source Codes

https://cran.r-project.org/web/packages/linkprediction/linkprediction.pdf

https://rpubs.com/writetosamadalvi/CommunityDetection

https://users.dimi.uniud.it/~massimo.franceschet/R/communities.html

https://rstudio-pubs-static.s3.amazonaws.com/734940_93adb495f0e34ca291fcda1d214129d1.html#Service_as_a_Freelancer









