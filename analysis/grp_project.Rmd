---
title: "Group Project (exploration)"
author: "group"
date: "2023-09-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(dplyr)
library(tidyr)
library(igraph)
library(data.table)
library(glue)
library(gridExtra)
library(ggplot2)
library(stringr)
library(here)
library(caret)
library(pROC)
```

```{r , setup, include=FALSE}
#set working directory to project root folder
knitr::opts_knit$set(root.dir = here())
```

**Project Background**

Established in 1995 as a pioneering online bookstore, Amazon has been a cherished destination for book enthusiasts for several decades. While the company has evolved and expanded into a diverse range of industries, it has consistently maintained its status as a go-to hub for book shoppers.

In the scope of this project, our objective is to delve into the evolving co-purchase patterns of books over time, understand it's characteristics and underlying patterns. With these information, we'll then develop a model that can provide personalized book recommendations based on books which the customer has added to his/her basket.

In doing so, we anticipate that our efforts will yield a mutually beneficial outcome. By facilitating book discoveries and guiding readers to titles they might not have encountered otherwise, we aim to stimulate sales on the platform, benefiting both Amazon and its thriving community of book lovers.

**1. Network Characteristics**

Note:

On our end, the metadata is collected in summer 2006 (possible dates is Wed, 21 Jun 2006 â€“ Sat, 23 Sept 2006), and our co-purchase data: `amazon0312` was collected in March 02 2003.

The number of months from the midpoint of summer, 7 August 2006, to March 02 2003 is 30 months 3 weeks, thus we round up to 31 months.

we've filtered for such book-to-book co-purchases:
1. With average reviews > 1 in 31 month period (for both books)
2. in a co-purchase, both books have a valid genre i.e., not an empty string

```{r read_data}
set.seed(123)

# Read the graph from the GraphML file
g <- read_graph(here("outputs/filtered_graph.graphml"), format = "graphml")
# read filtered book data
books_df <- read.csv(here("outputs/book_filtered.csv"))
# changing id column and vertex ids from 'int' to 'chr' dtype
books_df$id <- as.character(books_df$id)

g

head(books_df)
```

The code cells that are commented out were used to generated the baseline dataset.
```{r combinations}
# #generate combinations
# sample_nodes <- V(g) %>% as_ids()
# combinations <- combn(sample_nodes, 2, simplify = TRUE)
# 
# combi <- combinations[,1]
# combi
# 
# #function to check if a given pair of nodes are connected in a given graph
# check_connection <- function(combi) {
#   result <- are.connected(g, combi[1], combi[2])
#   return(result)
# }
# 
# #check if each pair of nodes are connected
# is_connected <- combinations %>% apply(., 2, check_connection)
# 
# df_edgelist = data.frame(V1 = combinations[1,], V2 = combinations[2,], is_connected = is_connected)
```

```{r combining_edgelist_with_data}
# df_edgelist_book <- df_edgelist %>% left_join(books_df %>% select(id, length, rating, reviews, salesrank) %>% rename(V1_length = length, V1_rating = rating, V1_reviews = reviews, V1_salesrank = salesrank), by  = c("V1" = "id")) %>% left_join(books_df %>% select(id, length, rating, reviews, salesrank) %>% rename(V2_length = length, V2_rating = rating, V2_reviews = reviews, V2_salesrank = salesrank), by  = c("V2" = "id"))
```

```{r aggreating_number_common_genres}
# 
# # First, split the genres for each unique node
# node_genres <- books_df %>%
#   select(id, cleaned_genres) %>%
#   distinct() %>%
#   mutate(genres_list = strsplit(cleaned_genres, "\\|"))
# 
# # Create a named list for fast lookup
# genre_list <- setNames(node_genres$genres_list, node_genres$id)
# 
# # Compute the number of common genres for each row in edgelist
# df_edgelist_book$num_common_genre <- mapply(function(v1, v2) {
#   length(intersect(genre_list[[as.character(v1)]], genre_list[[as.character(v2)]]))
# }, df_edgelist_book$V1, df_edgelist_book$V2)
# 
# # converting dependent variable, is_connected to binary variable
# df_edgelist_book$is_connected <- df_edgelist_book$is_connected * 1
# 
# #save csv
# write.csv(df_edgelist_book, "baseline_dataset.csv", row.names = FALSE)
```


```{r analysing_baseline_dataset}
baseline_df <- read.csv('baseline_dataset.csv')
class_proportions <- baseline_df %>% group_by(is_connected) %>% tally()
print(class_proportions)
# The dataset is highly imbalanced and this needs to be handled as if it is not dealt with properly, the algorithm will not be able to learn the characteristics of the minority class properly. Therefore, up-sampling or down-sampling techniques can be used to ensure equal or more balanced proportions between the two classes.

# Idea: Bin salesrank into 10 bins (?) and create into a categorical variable


# Code to split into training and test data
set.seed(123)  
# This function ensures automatically that class proportions are maintained
index <- createDataPartition(baseline_df$is_connected, p = 0.7, list = FALSE)
train_data <- baseline_df[index, ]
test_data <- baseline_df[-index, ]

# converting dependent variable to factor as many packages require to be in factor
train_data$is_connected <- as.factor(train_data$is_connected)
test_data$is_connected <- as.factor(test_data$is_connected)

print(paste("Number of positive class in train data:", sum(train_data$is_connected == 1)))

# downsampling of the majority class for TRAIN set only. Cannot touch TEST set.
downsampled_data <- downSample(train_data[, -which(names(train_data) == "is_connected")], train_data$is_connected)

# need to rename the "is_connected" column
colnames(downsampled_data)[colnames(downsampled_data) == "Class"] <- "is_connected"

print(paste("Number of positive class in downsampled data:", sum(downsampled_data$is_connected == 1)))
print(paste("Number of negative class in downsampled data:", sum(downsampled_data$is_connected == 0)))
```
```{r}
head(downsampled_data)



```
```{r creating_baseline_model}
# First create a baseline model: Logistic regression. It is chosen for its simplicity and easy interpretability. Come up with a baseline model first without any feature selection or hyperparameter tuning first and see performance.
baseline_logit_model <- glm(is_connected ~ ., data = downsampled_data, family = binomial(link = "logit"))

# Summary of the model
summary(baseline_logit_model)
```

```{r baseline_model_performance}
# predictions
predicted_probs <- predict(baseline_logit_model, newdata = test_data, type = "response")
threshold <- 0.5
predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)

# Confusion matrix
confusion <- confusionMatrix(as.factor(predicted_classes), as.factor(test_data$is_connected))
print(confusion)

# ROC curve and AUC
roc_obj <- roc(test_data$is_connected, predicted_probs)
auc(roc_obj)
plot(roc_obj, main="ROC curve", col="blue", lwd=2)

```


```{r node_attributes}

# store attributes into a data.frame
node_attributes_df=data.frame(vertex_id=V(g)$name, 
                              degree = degree(g),
                              closeness=closeness(g, mode='all'),
                              betweenness=betweenness(g, directed=FALSE),
                              transitivity=transitivity(g, type = "local"), # There are many NaN transitivity values; it means these nodes are isolated?
                              eigencentrality = eigen_centrality(g)$vector,
                              pagerank = page_rank(g)$vector,
                              component_membership = components(g)$membership) #should we specify weakly or strongly connected component?

# replacing NaN transitivity with value = 2 and treat it as a continuous variable first. Alternatively, we could also just one hot encode the transitivity column and treat it as a categorical variable
node_attributes_df$transitivity[is.na(node_attributes_df$transitivity)] <- 2

head(node_attributes_df)
```
```{r merging_baseline_dataset_node_attributes}
## downsampled_data vertex ids are currently in int format. Convert to chr for left_join(), then convert back to int before model training

downsampled_data_2 = downsampled_data

downsampled_data_2$V1 = as.character(downsampled_data_2$V1)
downsampled_data_2$V2 = as.character(downsampled_data_2$V2)

merged_data <- downsampled_data_2 %>% left_join(node_attributes_df %>% select(vertex_id, degree, closeness, betweenness, transitivity, eigencentrality, pagerank, component_membership) %>% rename(V1_degree = degree, V1_closeness = closeness, V1_betweenness = betweenness, V1_transitivity = transitivity, V1_eigencentrality = eigencentrality, V1_pagerank = pagerank, V1_component_membership = component_membership), by = c("V1" = "vertex_id")) %>% left_join(node_attributes_df %>% select(vertex_id, degree, closeness, betweenness, transitivity, eigencentrality, pagerank, component_membership) %>% rename(V2_degree = degree, V2_closeness = closeness, V2_betweenness = betweenness, V2_transitivity = transitivity, V2_eigencentrality = eigencentrality, V2_pagerank = pagerank, V2_component_membership = component_membership), by = c("V2" = "vertex_id"))

# converting V1 and V2 back to int
merged_data$V1 <- as.integer(merged_data$V1)
merged_data$V2 <- as.integer(merged_data$V2)
head(merged_data)
```

```{r merged_model_training}
merged_logit_model <- glm(is_connected ~ ., data = merged_data, family = binomial(link = "logit"))

# Summary of the model
summary(merged_logit_model)

```


```{r merged_model_performance}
# need to leftjoin test data set also
merged_test_data <- test_data
merged_test_data$V1 <- as.character(merged_test_data$V1)
merged_test_data$V2 <- as.character(merged_test_data$V2)

merged_test_data <- merged_test_data %>% left_join(node_attributes_df %>% select(vertex_id, degree, closeness, betweenness, transitivity, eigencentrality, pagerank, component_membership) %>% rename(V1_degree = degree, V1_closeness = closeness, V1_betweenness = betweenness, V1_transitivity = transitivity, V1_eigencentrality = eigencentrality, V1_pagerank = pagerank, V1_component_membership = component_membership), by = c("V1" = "vertex_id")) %>% left_join(node_attributes_df %>% select(vertex_id, degree, closeness, betweenness, transitivity, eigencentrality, pagerank, component_membership) %>% rename(V2_degree = degree, V2_closeness = closeness, V2_betweenness = betweenness, V2_transitivity = transitivity, V2_eigencentrality = eigencentrality, V2_pagerank = pagerank, V2_component_membership = component_membership), by = c("V2" = "vertex_id"))

merged_test_data$V1 <- as.integer(merged_test_data$V1)
merged_test_data$V2 <- as.integer(merged_test_data$V2)

# head(merged_test_data)

# predictions
merged_predicted_probs <- predict(merged_logit_model, newdata = merged_test_data, type = "response")
threshold <- 0.5
merged_predicted_classes <- ifelse(merged_predicted_probs > threshold, 1, 0)

# Confusion matrix
merged_confusion <- confusionMatrix(as.factor(merged_predicted_classes), as.factor(merged_test_data$is_connected))
print(merged_confusion)

# ROC curve and AUC
merged_roc_obj <- roc(merged_test_data$is_connected, merged_predicted_probs)
auc(merged_roc_obj)
plot(merged_roc_obj, main="ROC curve", col="blue", lwd=2)


```


```{r merging_node_attributes_books_df}
merged_df = books_df %>% left_join(node_attributes_df, by = c("id" = "vertex_id"))
head(merged_df)
```

**2.1. Degree Centrality**

The degree distribution of our network exhibits a power-law distribution, a characteristic often seen in naturally occurring networks. Majority of books in our network have low degree centrality, typically three or fewer connections. On the far right of the distribution, we observe a small group of books with 12 or more first-degree connections, indicating a significant influence on the purchase of other books.

This suggests that most books within the network neither strongly influence the purchase of other books nor are significantly influenced by other books. Examining these high in-degree centrality outliers can provide valuable insights, which could allow us to formulate effective cross-selling strategies that enhance co-purchase tendencies.

```{r deg_visualization, fig.width=15, fig.height=5}
#function to plot histogram
plot_hist <- function(g, title, x_label, y_label, log_scale = FALSE) {
  labels <- labs(y = y_label, x = x_label, title = title)  # Added title parameter
  graph <- ggplot() + geom_histogram(aes(x = g), bins = 10) + labels
  
  if (log_scale == TRUE) {
    return(graph + ls)
  } else {
    return(graph)
  }
}

#calculate in, out and all degree centrality for each graph
deg <- degree(g)

#visualize
plot_hist(deg, "degree distribution for 0302", "degree", "frequency")
```

**2.2. Degree Centrality**
The low network density of 0.000598 in the network indicates that, relative to the total possible connections between books, there are very few actual co-purchases. This suggests that the network is quite sparse, with a limited number of co-purchases taking place.

Global transitivity measures how likely the neighbors of two connected nodes are connected to each other. In the case of our co-purchase network, it measures the likelihood that the alters of two books which are co-purchased together are also co-purchased with each other. The moderate global transitivity value of 0.242 tells us that there is a noteworthy degree of clustering or transitivity within this sparse network. 

This means that, despite the overall sparsity, there is a tendency for co-purchased books to have shared co-purchases with other books in the network. In simpler terms, when two books are co-purchased together, there is a relatively higher likelihood that other books co-purchased with those two are also co-purchased with each other. 

This suggests that there are distinct co-purchase patterns or niche markets within the network. Products within these clusters frequently co-purchase with each other, even if the overall network is not densely interconnected.

```{r graph-attributes}
global_transitivity = transitivity(g, type="global")
density = graph.density(g)
print(paste("The global transitivity of the graph is", global_transitivity, "and the graph density is", density))
```

# Clusters

```{r component, fig.width = 15, fig.height = 10}
#get largest component
comps <- clusters(g)
largest_comp_ind <- which.max(comps$csize)
largest_comp_vert <- which(comps$membership == largest_comp_ind)
largest_comp_subg <- induced_subgraph(g, largest_comp_vert)

#visualize largest component
plot(largest_comp_subg, layout = layout.fruchterman.reingold(g), vertex.size = degree(largest_comp_subg))
#degree distribution for sampled graph
plot_hist(degree(g), "Degree Centrality distribution for sampled graph", "degree centrality", "frequency", log_scale = FALSE)
#degree distribution for largest component
plot_hist(degree(largest_comp_subg), "Degree Centrality distribution for largest component", "degree centrality", "frequency", log_scale = FALSE)
```


# Clustering

Source Codes

https://cran.r-project.org/web/packages/linkprediction/linkprediction.pdf

https://rpubs.com/writetosamadalvi/CommunityDetection

https://users.dimi.uniud.it/~massimo.franceschet/R/communities.html

https://rstudio-pubs-static.s3.amazonaws.com/734940_93adb495f0e34ca291fcda1d214129d1.html#Service_as_a_Freelancer









